{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnvGGZKNG3IR",
    "outputId": "efe1a56e-bac5-43c1-eda4-e8bbc19109ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n",
      "Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "lN1EmruwHKhd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "import sys, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName('A')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "eBq6X5xyH-l9"
   },
   "outputs": [],
   "source": [
    "# 2.3.1. Умножение матрицы на вектор с применением MapReduce\n",
    "def mult_matrix_vector(mat_rdd, vect_rdd):\n",
    "    qty = mat_rdd.keys().distinct().count()\n",
    "    print(\"qty\", qty)\n",
    "    rdd1 = vect_rdd.flatMap(lambda x: [((j, x[0]), (x[1])) for j in range(qty)])\n",
    "    print(\"rdd1\", rdd1.take(10))\n",
    "    rdd2 = mat_rdd + rdd1\n",
    "    print(\"rdd2\", rdd2.take(10))\n",
    "    result = rdd2.reduceByKey(lambda x, y: x * y).map(lambda x: (x[0][0], x[1])).reduceByKey(lambda x, y: x + y)\n",
    "    print(\"result\", result.take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLee7rLQInPk",
    "outputId": "8441e243-f0e7-42e1-aea6-eba04f149ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat_rdd [((0, 0), 6.0), ((0, 1), 4.0), ((0, 2), 3.0), ((0, 3), 3.0), ((0, 4), 1.0), ((0, 5), 3.0), ((0, 6), 5.0), ((0, 7), 3.0), ((0, 8), 6.0), ((0, 9), 9.0), ((1, 0), 5.0), ((1, 1), 1.0), ((1, 2), 9.0), ((1, 3), 4.0), ((1, 4), 5.0), ((1, 5), 9.0), ((1, 6), 10.0), ((1, 7), 8.0), ((1, 8), 7.0), ((1, 9), 3.0)]\n",
      "vect_rdd [(0, 10.0), (1, 10.0), (2, 10.0), (3, 10.0), (4, 10.0), (5, 10.0), (6, 10.0), (7, 10.0), (8, 10.0), (9, 10.0)]\n"
     ]
    }
   ],
   "source": [
    "mat_rdd = spark.read.csv('matrix.csv', header=True).rdd\n",
    "mat_rdd = mat_rdd.map(lambda x: ((int(x[0]), int(x[1])), float(x[2])))\n",
    "print(\"mat_rdd\", mat_rdd.take(20))\n",
    "vect_rdd = spark.read.csv('vector.csv', header=True).rdd\n",
    "vect_rdd = vect_rdd.map(lambda x: (int(x[0]), float(x[1])))\n",
    "print(\"vect_rdd\", vect_rdd.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLhfaZVwIpNs",
    "outputId": "b5c2dc02-2c5e-4e18-883b-73b06b7dd29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qty 100\n",
      "rdd1 [((0, 0), 10.0), ((1, 0), 10.0), ((2, 0), 10.0), ((3, 0), 10.0), ((4, 0), 10.0), ((5, 0), 10.0), ((6, 0), 10.0), ((7, 0), 10.0), ((8, 0), 10.0), ((9, 0), 10.0)]\n",
      "rdd2 [((0, 0), 6.0), ((0, 1), 4.0), ((0, 2), 3.0), ((0, 3), 3.0), ((0, 4), 1.0), ((0, 5), 3.0), ((0, 6), 5.0), ((0, 7), 3.0), ((0, 8), 6.0), ((0, 9), 9.0)]\n",
      "result [(0, 430.0), (2, 660.0), (4, 700.0), (6, 670.0), (8, 750.0), (10, 100.0), (12, 100.0), (14, 100.0), (16, 100.0), (18, 100.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1060] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_matrix_vector(mat_rdd, vect_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "PgmW7BPuPYIK"
   },
   "outputs": [],
   "source": [
    "# 2.3.4. Вычисление выборки с помощью MapReduce\n",
    "def selection(sel_rdd, condition):\n",
    "    rdd1 = sel_rdd.map(lambda x: (int(x[0]), x[1]))\n",
    "    print(\"rdd1\", rdd1.take(10))\n",
    "    rdd2 = rdd1.flatMap(lambda x: [(x[1], x[1])] if condition(x[1]) else [])\n",
    "    print(\"rdd2\",  rdd2.take(10))\n",
    "    result = rdd2.reduceByKey(lambda x, y: x)\n",
    "    print(\"result\", result.take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqxAscrdQSYR",
    "outputId": "aafa55bd-a276-45d4-d1a7-8dc3b220003e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sel_rdd [Row(value='1', c1='X'), Row(value='2', c1='A'), Row(value='3', c1='Q'), Row(value='4', c1='E'), Row(value='5', c1='Y'), Row(value='6', c1='K'), Row(value='7', c1='R'), Row(value='8', c1='A'), Row(value='9', c1='B')]\n"
     ]
    }
   ],
   "source": [
    "sel_rdd = spark.read.csv(\"abc.csv\", header=True).rdd\n",
    "print(\"sel_rdd\", sel_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cT61urjfQi3q",
    "outputId": "b00dce02-a691-47a1-cec9-5e7b6de712e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 [(1, 'X'), (2, 'A'), (3, 'Q'), (4, 'E'), (5, 'Y'), (6, 'K'), (7, 'R'), (8, 'A'), (9, 'B')]\n",
      "rdd2 [('X', 'X'), ('Y', 'Y')]\n",
      "result [('X', 'X'), ('Y', 'Y')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1084] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection(sel_rdd, lambda x: x == 'X' or x == 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "cPWCyBBESmRR"
   },
   "outputs": [],
   "source": [
    "# 2.3.5. Вычисление проекции с помощью MapReduce\n",
    "def projection(projection_rdd, index):\n",
    "    rdd1 = projection_rdd.map(lambda x: (tuple([x[i] for i in index]), tuple([x[i] for i in index])))\n",
    "    print(\"rdd1\", rdd1.take(10))\n",
    "    result = rdd1.reduceByKey(lambda x, y: x)\n",
    "    print(\"result\", result.take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZN_YkVQTSua4",
    "outputId": "c798b07f-3a9a-43e3-dc6e-470c98982d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projection_rdd = [Row(id='0', value='21', class1='A', class2='A'), Row(id='1', value='22', class1='W', class2='T'), Row(id='2', value='23', class1='Q', class2='E'), Row(id='3', value='24', class1='E', class2='R'), Row(id='4', value='25', class1='R', class2='A'), Row(id='5', value='26', class1='B', class2='B'), Row(id='6', value='77', class1='C', class2='C'), Row(id='7', value='88', class1='A', class2='C'), Row(id='8', value='99', class1='B', class2='B')]\n"
     ]
    }
   ],
   "source": [
    "projection_rdd = spark.read.csv(\"proj.csv\", header=True).rdd\n",
    "print(\"projection_rdd =\", projection_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDsyh-FuSwPg",
    "outputId": "6f6c5528-fca7-47a3-b254-70e223f77ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 [(('A', 'A'), ('A', 'A')), (('W', 'W'), ('W', 'W')), (('Q', 'Q'), ('Q', 'Q')), (('E', 'E'), ('E', 'E')), (('R', 'R'), ('R', 'R')), (('B', 'B'), ('B', 'B')), (('C', 'C'), ('C', 'C')), (('A', 'A'), ('A', 'A')), (('B', 'B'), ('B', 'B'))]\n",
      "result [(('A', 'A'), ('A', 'A')), (('W', 'W'), ('W', 'W')), (('Q', 'Q'), ('Q', 'Q')), (('E', 'E'), ('E', 'E')), (('R', 'R'), ('R', 'R')), (('B', 'B'), ('B', 'B')), (('C', 'C'), ('C', 'C'))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1107] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection(projection_rdd, [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnc9JqQgVC3a",
    "outputId": "b330be71-a2a6-454d-e20a-e0bb4b44ef40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_rdd [Row(i='0', value='10'), Row(i='1', value='10'), Row(i='2', value='20'), Row(i='3', value='30'), Row(i='4', value='40')]\n",
      "second_rdd [Row(i='0', value='30'), Row(i='1', value='40'), Row(i='2', value='50'), Row(i='3', value='60'), Row(i='4', value='90')]\n"
     ]
    }
   ],
   "source": [
    "first_rdd = spark.read.csv(\"1st.csv\", header=True).rdd\n",
    "print(\"first_rdd\", first_rdd.take(10))\n",
    "second_rdd = spark.read.csv(\"2nd.csv\", header=True).rdd\n",
    "print(\"second_rdd\", second_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "1B72xWXqWIXA"
   },
   "outputs": [],
   "source": [
    "# 2.3.6. Вычисление объединения с помощью MapReduce\n",
    "def union(first_rdd, second_rdd):\n",
    "    rdd1 = first_rdd + second_rdd\n",
    "    print(\"rdd1\", rdd1.take(10))\n",
    "    rdd2 = rdd1.map(lambda x: (x[1], x[1]))\n",
    "    print(\"rdd2\", rdd2.take(10))\n",
    "    result = rdd2.reduceByKey(lambda x, y: x)\n",
    "    print(\"result\", result.take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXom7TMSWJ-3",
    "outputId": "54f0d7df-d239-432c-d847-d90a8bf674c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 [Row(i='0', value='10'), Row(i='1', value='10'), Row(i='2', value='20'), Row(i='3', value='30'), Row(i='4', value='40'), Row(i='0', value='30'), Row(i='1', value='40'), Row(i='2', value='50'), Row(i='3', value='60'), Row(i='4', value='90')]\n",
      "rdd2 [('10', '10'), ('10', '10'), ('20', '20'), ('30', '30'), ('40', '40'), ('30', '30'), ('40', '40'), ('50', '50'), ('60', '60'), ('90', '90')]\n",
      "result [('10', '10'), ('20', '20'), ('40', '40'), ('50', '50'), ('60', '60'), ('30', '30'), ('90', '90')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1151] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union(first_rdd, second_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "iZ7RXViGWgtn"
   },
   "outputs": [],
   "source": [
    "# 2.3.6. Вычисление пересечения с помощью MapReduce\n",
    "def intersection(first_rdd, second_rdd):\n",
    "    first_rdd = first_rdd.map(lambda x: (x[1], '1'))\n",
    "    second_rdd = second_rdd.map(lambda x: (x[1], '2'))\n",
    "    r = first_rdd + second_rdd\n",
    "    result = r.groupByKey().flatMap(lambda x: [(x[0], set(tuple(x[1])))])\n",
    "    result = result.flatMap(lambda x: [x[0]] if len(x[1]) > 1 else [])\n",
    "    print(\"result\", result.take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUyRCkTHWlVH",
    "outputId": "f15949cc-b8f9-44bf-80c4-d886188e9b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result ['40', '30']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1161] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(first_rdd, second_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "3Zo9mhMCYto-"
   },
   "outputs": [],
   "source": [
    "# 2.3.6. Вычисление разности с помощью MapReduce\n",
    "def difference(first_rdd, second_rdd):\n",
    "    a_rdd2 = first_rdd.map(lambda x: (x[1], 0))\n",
    "    print(\"a_rdd2\", a_rdd2.take(10))\n",
    "    b_rdd2 = second_rdd.map(lambda x: (x[1], 1))\n",
    "    print(\"b_rdd2\", b_rdd2.take(10))\n",
    "    rdd1 = a_rdd2 + b_rdd2\n",
    "    print(\"rdd1\", rdd1.take(10))\n",
    "    group = rdd1.groupByKey()\n",
    "    print(\"group\", group.take(10))\n",
    "    result = group.flatMap(lambda x: [(x[0], x[0])] if sum(x[1]) == 0 else [])\n",
    "    #print(\"result\", result.take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d_BrZcVY3I1",
    "outputId": "cadf2690-9571-482e-95b0-9b3183df17a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_rdd2 [('10', 0), ('10', 0), ('20', 0), ('30', 0), ('40', 0)]\n",
      "b_rdd2 [('30', 1), ('40', 1), ('50', 1), ('60', 1), ('90', 1)]\n",
      "rdd1 [('10', 0), ('10', 0), ('20', 0), ('30', 0), ('40', 0), ('30', 1), ('40', 1), ('50', 1), ('60', 1), ('90', 1)]\n",
      "group [('10', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31a10>), ('20', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31610>), ('40', <pyspark.resultiterable.ResultIterable object at 0x7f4963f312d0>), ('50', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31710>), ('60', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31c50>), ('30', <pyspark.resultiterable.ResultIterable object at 0x7f4963f29b10>), ('90', <pyspark.resultiterable.ResultIterable object at 0x7f4963f29310>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1175] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference(first_rdd, second_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tHEujvNcX3N",
    "outputId": "2ed4feff-f005-4f21-9ecd-15f4381da2f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join1_rdd [(0, ('1', 'A')), (1, ('2', 'C')), (2, ('3', 'B')), (3, ('4', 'Q')), (4, ('5', 'W')), (5, ('6', 'S')), (6, ('7', 'A')), (7, ('8', 'B')), (8, ('9', 'D'))]\n",
      "join2_rdd [(0, ('A', 'QW')), (1, ('B', 'WE')), (2, ('Q', 'ER')), (3, ('E', 'RT')), (4, ('R', 'TY')), (5, ('T', 'YU')), (6, ('N', 'UI')), (7, ('G', 'IO')), (8, ('L', 'OP'))]\n"
     ]
    }
   ],
   "source": [
    "join1_rdd = spark.read.csv('join1.csv', header=True).rdd\n",
    "join1_rdd = join1_rdd.map(lambda x: (int(x[0]), (x[1], x[2])))\n",
    "print(\"join1_rdd\", join1_rdd.take(10))\n",
    "join2_rdd = spark.read.csv('join2.csv', header=True).rdd\n",
    "join2_rdd = join2_rdd.map(lambda x: (int(x[0]), (x[1], x[2])))\n",
    "print(\"join2_rdd\", join2_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "X9W3xOBocrb0"
   },
   "outputs": [],
   "source": [
    "# 2.3.7. Вычисление естественного соединения с помощью MapReduce\n",
    "import itertools\n",
    "def join(a_rdd, b_rdd):\n",
    "    def combine(l):\n",
    "        a = []\n",
    "        b = []\n",
    "        for item in l:\n",
    "            if item[0] == 0:\n",
    "                a += [item[1]]\n",
    "            else:\n",
    "                b += [item[1]]\n",
    "        return list(itertools.product(a, b))\n",
    "    a_rdd2 = a_rdd.map(lambda x: (x[1][1], (0, x[1][0])))\n",
    "    print(\"a_rdd2\", a_rdd2.take(10))\n",
    "    b_rdd2 = b_rdd.map(lambda x: (x[1][0], (1, x[1][1])))\n",
    "    print(\"b_rdd2\", b_rdd2.take(10))\n",
    "    rdd1 = a_rdd2 + b_rdd2\n",
    "    print(\"rdd1\", rdd1.take(10))\n",
    "    group = rdd1.groupByKey()\n",
    "    print(\"group\", group.take(10))\n",
    "    result = group.flatMap(lambda x: [(x[0], item) for item in combine(list(x[1]))])\n",
    "    print(\"result\", result.take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GM4ngsZocuS9",
    "outputId": "413654ca-0fbd-4794-9fd2-c0f5b8055823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_rdd2 [('A', (0, '1')), ('C', (0, '2')), ('B', (0, '3')), ('Q', (0, '4')), ('W', (0, '5')), ('S', (0, '6')), ('A', (0, '7')), ('B', (0, '8')), ('D', (0, '9'))]\n",
      "b_rdd2 [('A', (1, 'QW')), ('B', (1, 'WE')), ('Q', (1, 'ER')), ('E', (1, 'RT')), ('R', (1, 'TY')), ('T', (1, 'YU')), ('N', (1, 'UI')), ('G', (1, 'IO')), ('L', (1, 'OP'))]\n",
      "rdd1 [('A', (0, '1')), ('C', (0, '2')), ('B', (0, '3')), ('Q', (0, '4')), ('W', (0, '5')), ('S', (0, '6')), ('A', (0, '7')), ('B', (0, '8')), ('D', (0, '9')), ('A', (1, 'QW'))]\n",
      "group [('C', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31e50>), ('W', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31210>), ('S', <pyspark.resultiterable.ResultIterable object at 0x7f4963f7a550>), ('R', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31ed0>), ('N', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31810>), ('L', <pyspark.resultiterable.ResultIterable object at 0x7f4963f31050>), ('A', <pyspark.resultiterable.ResultIterable object at 0x7f4963f400d0>), ('B', <pyspark.resultiterable.ResultIterable object at 0x7f4963f40ed0>), ('Q', <pyspark.resultiterable.ResultIterable object at 0x7f4963f405d0>), ('D', <pyspark.resultiterable.ResultIterable object at 0x7f4963f40850>)]\n",
      "result [('A', ('1', 'QW')), ('A', ('7', 'QW')), ('B', ('3', 'WE')), ('B', ('8', 'WE')), ('Q', ('4', 'ER'))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1223] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join(join1_rdd, join2_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpwynWSXe1Qb",
    "outputId": "262202b5-effd-4af5-ea6f-1758c302e3c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agr_rdd [Row(id='0', class='A', value='1'), Row(id='1', class='B', value='2'), Row(id='2', class='C', value='3'), Row(id='3', class='A', value='1'), Row(id='4', class='B', value='2'), Row(id='5', class='C', value='3'), Row(id='6', class='A', value='1')]\n"
     ]
    }
   ],
   "source": [
    "agr_rdd = spark.read.csv(\"agr.csv\", header=True).rdd\n",
    "print(\"agr_rdd\", agr_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "XEanBDlBeiCs"
   },
   "outputs": [],
   "source": [
    "# 2.3.8. Вычисление группировки и агрегирования с помощью MapReduce\n",
    "def aggregate(agg_rdd, aggregator):\n",
    "    rdd1 = agg_rdd.map(lambda x: (x[1], float(x[2])))\n",
    "    print(\"rdd1\", rdd1.take(10))\n",
    "    result  = rdd1.reduceByKey(lambda x, y: aggregator(x, y))\n",
    "    print(\"result\", result .take(10))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFL_uLQte3pk",
    "outputId": "a3de404c-31ee-4aee-dc5e-92e6b5bb2ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 [('A', 1.0), ('B', 2.0), ('C', 3.0), ('A', 1.0), ('B', 2.0), ('C', 3.0), ('A', 1.0)]\n",
      "result [('A', 3.0), ('B', 4.0), ('C', 6.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1246] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate(agr_rdd, lambda x, y: x + y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Big Data раздел 2 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
